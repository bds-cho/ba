\section{Benchmark Design}
\label{cha:approach}

In this section, we present our approach to benchmark AWS compute and datastore service pairs, while addressing requirements and challenges discussed in \cref{cha:background}. Affordability is a major requirement in this thesis, as the whole experiment is conducted within the limits of AWS Free Tier\footcite{https://aws.amazon.com/free/}. This influences design and implementation decisions while necessitating trade-offs with other requirements.

\cref{cha:intro,cha:background} highlight the importance of access latency in applications and it is the primary metric of interest in this thesis. To isolate access latency, we focus on read-only operations between compute and datastore services, to avoid locking and transactional overheads in writes. Queries are kept simple, database tables, and object files minimal to reduce querying and data transmission overhead. While mixed read-write workloads, larger databases, and complex queries reflect real-world scenarios, simple read-only benchmarks can provide initial baseline latency performance. Additionally, focusing on reads is cost-effective, as services like S3 offer significantly more free-tier reads compared to writes.

We evaluate and compare six pairs of prominent AWS compute and datastore services for access latency: (1) EC2-RDS, (2) EC2-DynamoDB, (3) EC2-S3, (4) Lambda-RDS, (5) Lambda-DynamoDB, and (6) Lambda-S3. Each benchmarking run involves two primary components: (1) a pre-loaded datastore serving as the System Under Test (SUT) and (2) a compute instance which generates workload on the SUT and collects access latency data. For simplicity, we refer to the datastore as SUT and the compute instance as workload generator in this thesis. However, in reality, a compute instance and a datastore together constitute one SUT, as we are not particularly testing the datastores.

To address relevance, we employ open workload model, due to the reasons discussed in \cref{elems_of_bench} and evaluate each pair under two distinct workload types: constant and burst. Under constant workload, the reads per second (RPS) remain stable throughout the benchmarking run. A burst workload, on the other hand, simulates sudden spikes in RPS, reflecting real-world scenarios where workload can be unpredictable. This allows us to observe how each pair performs under steady load and how they respond to fluctuating, high-demand conditions. To address repeatability we evaluate each pair under both workload types twice, resulting in total of 24 runs.

As discussed in \cref{challenges}, the fluctuating underlying conditions of the cloud can impact performance. To address this challenge, we extend the durations of benchmarking runs as much as possible and repeat them at different times and days, while balancing affordability and relevance. Fairness is ensured by maintaining identical workloads, resource allocation, network conditions, and datastore configurations across service pairs.
