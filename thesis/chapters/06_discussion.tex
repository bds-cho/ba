\section{Discussion}
\label{cha:discuss}

\cref{cha:results} shows that datastore access latency is influenced by the choice of compute service, with EC2 consistently outperforming Lambda across all experiments. This suggests an inherent variance and overhead associated with Lambda. In this section, we explore possible reasons for these observations and also assess our approach and its limitations.

In \cref{fig:ts-plots}, we observe temporal shifts in the latency performance, even under constant workload. Temporal performance variability in serverless platforms, including AWS Lambda, is well-documented \cite{paper_ginzburg_lambda_var,article_eismann_lambda_var,paper_schirmer_night_shift}. Ginzburg et al. \cite{paper_ginzburg_lambda_var} report significant performance shifts in AWS Lambda during a one-week continuous benchmark, even suggesting that these variations could be exploited to reduce costs. It is possible that our benchmarking runs, specifically Lambda-RDS (\hyperref[fig:bar_rds_const]{Figure 5.2.1} and \hyperref[fig:bar_rds_bursty]{Figure 5.2.2}) and Lambda-Dynamo (\hyperref[fig:bar_ddb_const]{Figure 5.2.3} and \hyperref[fig:bar_ddb_bursty]{Figure 5.2.4}), were affected by such temporal shifts, impacting the results. Similar is also applicable for EC2 \cite{paper_iosup_performance,article_schad_cloud_var}, however, Dancheva et al. \cite{article_dancheva_ec2_var} suggest that recent advances are able to improve networking performance in EC2.

Lambda functions run inside microVMs based on Firecracker\footcite{https://firecracker-microvm.github.io/} hypervisor, deployed on bare-metal, multi-tenant EC2 instances referred as Lambda-Workers. Unlike standard EC2 instances based on the AWS Nitro\footcite{https://aws.amazon.com/ec2/nitro/} hypervisor, Lambda-Workers add Firecracker as an additional abstraction layer. Communication between a microVM and Firecracker occurs via optimized virtio\footcite{https://libvirt.org/} interface \cite{paper_brooker_lambda}, leading to overall networking overhead of approximately $0.06ms$ under controlled, isolated conditions \cite{repo_aws_firecracker}. In production, however, Lambda-Workers may host hundreds or thousands of microVMs \cite{paper_agache_firecracker}, potentially increasing this overhead due to induced network noise and resource contention \cite{}. Agache et al. \cite{paper_agache_firecracker} also highlight Firecracker's comparatively lower networking performance against other hypervisors. Furthermore, Lambda-Workers are hosted within a network-isolated VPC managed by Lambda in the service accounts inaccessible to customers \cite{web_aws_lambda_security}, leaving their precise network configuration unclear.

--> \hl{Did it go as planned? Limitations? Improvements?} <--
In \cref{cha:background} we outlined general aspects and requirements of cloud benchmarking, we presented our approach in \cref{cha:approach}, and consequently we successfully evaluated through experimentation. The constraints of the AWS Free Tier restricted the duration and scale of experiments, limiting the observation of long-term performance trends. The reliance on low-tier configurations, such as t3.micro instances, prevented evaluation in realistic production environments where higher-capacity instances are standard. Temporal variability in cloud performance was another challenge, with short-duration benchmarks potentially not capturing the real empirical distribution of the access latency. Additionally, the absence of multi-region setups meant geo-distributed latency behaviors were not analyzed.
