\section{Discussion}
\label{cha:discuss}

\cref{cha:results} shows that datastore access latency is influenced by the choice of compute service, with EC2 consistently outperforming Lambda across all experiments. This suggests an inherent variance and overhead associated with Lambda. In this section, we explore possible reasons for these observations and also assess our approach and its limitations.

In \cref{fig:ts-plots}, we observe temporal shifts in the latency performance, even under constant workload. Temporal performance variability in serverless platforms, including AWS Lambda, is well-documented \cite{paper_ginzburg_lambda_var,article_eismann_lambda_var,paper_schirmer_night_shift}. Ginzburg et al. \cite{paper_ginzburg_lambda_var} report significant performance shifts in AWS Lambda during a one-week continuous benchmark, even suggesting that these variations could be exploited to reduce costs. It is possible that our benchmarking runs, specifically Lambda-RDS (\hyperref[fig:bar_rds_const]{Figure 5.2.1} and \hyperref[fig:bar_rds_bursty]{5.2.2}) and Lambda-Dynamo (\hyperref[fig:bar_ddb_const]{Figure 5.2.3} and \hyperref[fig:bar_ddb_bursty]{ 5.2.4}), were affected by such temporal shifts, impacting the results. Similar is also applicable for EC2 \cite{paper_iosup_performance,article_schad_cloud_var}, however, Dancheva et al. \cite{article_dancheva_ec2_var} suggest that recent advances are able to improve networking performance in EC2.

Lambda functions run inside microVMs based on Firecracker\footcite{https://firecracker-microvm.github.io/} hypervisor, deployed on bare-metal, multi-tenant EC2 instances referred as Lambda-Workers. Unlike standard EC2 instances based on the AWS Nitro\footcite{https://aws.amazon.com/ec2/nitro/} hypervisor, Lambda-Workers add Firecracker as an additional abstraction layer. Communication between a microVM and Firecracker occurs via optimized virtio\footcite{https://libvirt.org/} interface \cite{paper_brooker_lambda}, leading to overall networking overhead of approximately $0.06ms$ under controlled, isolated conditions \cite{repo_firecracker}. In production, however, Lambda-Workers may host hundreds or thousands of microVMs \cite{paper_agache_firecracker}, potentially increasing this overhead due to induced network noise and resource contention \cite{article_desensi_noise,paper_wang_faas_bts}. Agache et al. \cite{paper_agache_firecracker} also highlight Firecracker's comparatively lower networking performance against other hypervisors. Furthermore, Lambda-Workers are hosted within a network-isolated VPC managed by Lambda in the service accounts inaccessible to customers \cite{web_aws_lambda_security}, leaving their precise network configuration unclear.

We observe a minimum latency of $0ms$ in \hyperref[fig:bar_rds_const]{Figure 5.1.1} and \hyperref[fig:bar_rds_bursty]{5.2.2}, which is due to the $1ms$ measurement accuracy, causing latencies below $1ms$ to be rounded to $0ms$. As the differences are small, on the microsecond level, high-resolution timestamps with $1\mu s$ accuracy are better suited for more precise results.

\paragraph*{Limitations} In \cref{cha:background}, we outline general aspects and requirements of cloud benchmarking, present our approach in \cref{cha:approach}, and validate it through experimentation. However, affordability emerges as a significant limitation. AWS Free Tier constraints restrict experiment duration and scale, limiting long-term performance trend observations. The use of low-tier configurations, such as t2.micro instances and no provisioned concurrency for Lambda, hinder evaluation under realistic production conditions. Temporal variability in cloud performance poses another challenge, with short-duration benchmarks, particularly for S3, possibly failing to capture the real empirical distribution of access latency. The absence of multi-region setups excludes the analysis of geo-distributed latency behaviors.
