\section{Benchmark Design}
\label{cha:approach}

To address the main question, we adopt a benchmarking approach to evaluate the pairs and compare their results. This section presents the benchmark design, which is based on the guidelines outlined in \cite{}. As described in \cref{cha:background}, the study examines six pairs of AWS compute and datastore services:

\begin{itemize}
	\item EC2 paired with RDS, DynamoDB, and S3
	\item Lambda paired with RDS, DynamoDB, and S3
\end{itemize}

Each benchmarking session consists of two main components: (1) a pre-loaded datastore serving as the System Under Test (SUT) and (2) a compute instance functioning as the load generator. The compute instance performs read operations on the datastore and records latency metrics.

For Lambda-Pairs, each invocation corresponds to a single read operation on the datastore. To invoke Lambda in a scripted manner and collect results, an additional EC2 instance, referred to as the "Lambda-Helper", is provisioned with k6 and the necessary scripts, as shown in \ref{}. This instance sends HTTP requests to invoke the Lambda function and collects response data, which includes timestamps recorded by Lambda.

The benchmark evaluates each pair under two load modes, with each configuration tested twice, resulting in a total of 24 sessions. We use Terraform and shell scripts for resource provisioning and de-provisioning in AWS, ensuring a high level of automation to minimize human error. All sessions are conducted in the same AWS region.

Post-session, data is exported to a dedicated S3 bucket for data collection. Finally, we transform collected data and perform analysis to gain insights.
